{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40ffef0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading Farms to Freeways from the API and ro-crate metadata file\n",
    "\n",
    "The Language Data Commons of Australia (LDaCa) packages all their data collections in an [ro-crate](https://www.researchobject.org/ro-crate/). There is a metadata file called `ro-crate-metadata.json` that comes with every data collection and this is how we can obtain metadata on this collection of research objects.\n",
    "\n",
    "The metadata file is in the json format, and so we'll be learning how to read a json file in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdbb9b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Skills</b> \n",
    "    \n",
    "<ul>\n",
    "<li> json file format (see https://en.wikipedia.org/wiki/JSON)</li>\n",
    "<li> working with dataframes, via pandas</li>\n",
    "<li> discovering and exploring metadata</li>\n",
    "<li> extracting ngrams, via textacy</li>\n",
    "</ul>    \n",
    "<br>\n",
    "\n",
    "<b>Skill level:</b> Intermediate\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a0f7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook uses the library 'requests', as shown in the [Using APIs: Open Australia](https://github.com/Australian-Text-Analytics-Platform/open-australia-api/blob/main/api.ipynb) notebook. If you haven't already familiarised yourself with that notebook, it might be a good idea to do so first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5df6d3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Before we begin, let's make sure that we install all the requirements that we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed4bbe5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import libraries\n",
    "\n",
    "Python needs the libraries that will be used by the notebook to be specified before they are used. We do this with the reserved word `import`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8797d7c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json                       # json library to read json file formats\n",
    "import pprint                     # Prints in a nice way\n",
    "import requests                   # Uses the requests library for REST apis\n",
    "import os                         # Loads operating system libraries\n",
    "from ldaca.ldaca import LDaCA     # Loads the LDaCA ReST api wrapper\n",
    "from rocrate_lang.utils import as_list # A handy utility for converting to list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9651e6a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Variables \n",
    "\n",
    "We need to specify the path to the data collection. This `Farms to Freeways` data collection was used with permission by [The University of Western Sydney](https://omeka.uws.edu.au/farmstofreeways/). It was made into an ro-crate by the [LDaCA](https://ardc.edu.au/project/language-data-commons-of-australia-ldaca/) project and it is the data set used here to demonstrate the skills list above.\n",
    "\n",
    "The variables below refer to the `path` where the collection can be found. There are also variables below that refer to ro-crates as specified in LDaCA profiles, for example all artefacts of importances are called `RepositoryObject`, and when an artefact is linked to, it is done with a `hasFile` keyword in the ro-crate metadata file.\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "Create a file, name it vars.env and store your API_KEY. This will be required for downloading any files. Go to the <a href='https://data.atap.edu.au/'>ATAP LDaCA Portal</a> and generate an API Key\n",
    "\n",
    "Example vars.env:\n",
    "\n",
    "API_KEY=12345\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5dbaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify location where collection is\n",
    "LDACA_API = 'https://data.atap.edu.au/api'\n",
    "COLLECTION_ID = 'arcp://name,farms-to-freeways-example-dataset'\n",
    "from dotenv import load_dotenv    # loads environment variables\n",
    "load_dotenv('vars.env') # load the environment variables located in the vars.env files\n",
    "API_TOKEN = os.getenv('API_KEY') # store your environment variable in this jupyter notebook\n",
    "if not API_TOKEN:\n",
    "    print(\"Set a variable in the vars.env file and name API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb53776",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Farms to Freeways ro-crate metadata by passing the arcpId in as a parameter to the get request\n",
    "\n",
    "ldaca = LDaCA(url=LDACA_API, token=API_TOKEN, data_dir='data')\n",
    "ldaca.retrieve_collection(collection=COLLECTION_ID, collection_type='Collection', data_dir='data')\n",
    "\n",
    "metadata = ldaca.crate\n",
    "\n",
    "# Inspect the metadata (Currently commented out for brevity)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fc57b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ro-crate Profiles\n",
    "\n",
    "An ro-crate profile is a set of conventions that tell us what elements an ro-crate minimally contains.\n",
    "\n",
    "These profiles tell us what to expect to find in the data packages. Learn more about them here: https://www.researchobject.org/ro-crate/profiles.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8856b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TYPE values should be lists. \n",
    "# We define a PRIMARY_OBJECT as a 'RepositoryObject' because that is where the main data is stored \n",
    "PRIMARY_OBJECT = 'RepositoryObject'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc7f87",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define Variables to Gather Metadata\n",
    "\n",
    "As suggested above, there are '@types' that define certain objects within the collection, for example there is type called 'Person'. This json object stores information such as 'birthDate' about the 'Person'. In the code block below, we discover all the types stored in this metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcec6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Find all types and find types that have linked objects\n",
    "files = set()\n",
    "types = list()\n",
    "primary_object_types = list()\n",
    "\n",
    "# Lets see what we can find in our metadata\n",
    "for entity in ldaca.crate.contextual_entities + ldaca.crate.data_entities:\n",
    "    entity_type = as_list(entity.type)  # We make sure that each type is a list\n",
    "    for e_t in entity_type:\n",
    "        types.append(e_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613ed17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploring the Metadata\n",
    "\n",
    "Anytime you work with data, it's always a good idea to inspect it by printing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64dc7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the variables\n",
    "# All the types, removing duplicates\n",
    "list(dict.fromkeys(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a041dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Primary Objects\n",
    "\n",
    "The primary object types are the ones we may care about, so we will pull them into their own dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718350b4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use the special function called `ldaca.crate.dereference(id)` to find out which linked object this is\n",
    "More on how to consume an ro-crate using python here: https://github.com/ResearchObject/ro-crate-py#consuming-an-ro-crate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae67a01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Types of PRIMARY_OBJECTs ie [PRIMARY_OBJECT, X]. What kinds of Xs do we have?\n",
    "for entity in ldaca.crate.contextual_entities + ldaca.crate.data_entities:\n",
    "    if 'RepositoryObject' in as_list(entity.type):\n",
    "        print(entity.get('name'))\n",
    "        item = ldaca.crate.dereference(entity.id)\n",
    "        primary_object_types.append(item.as_jsonld())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b540b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: pandas (dataframe)</b> \n",
    "<br>    \n",
    "A dataframe is akin to a table -- it is made up of rows and columns. \n",
    "<br>    \n",
    "In the block of code below, we are creating a dataframe for each \"primary_object_type\" ('Person', 'TextDialogue', 'Photographic image', and 'Text')\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67896d7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # this means we will refer to pandas as 'pd' throughout the code\n",
    "\n",
    "primary_objects_dataframe = pd.json_normalize(primary_object_types)\n",
    "primary_objects_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1560381",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## hasPart\n",
    "\n",
    "Each RepositoryObject has a list of files in an array called `hasPart`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7f804",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Types of File that are in each primary object. What kinds of files do we have?\n",
    "for entity in primary_object_types:\n",
    "    if 'hasPart' in entity:\n",
    "        hasPart = entity.get('hasPart')\n",
    "        for part in as_list(hasPart):\n",
    "            file = ldaca.crate.dereference(part.get('@id'))\n",
    "            files.add(file)\n",
    "print(f\"{len(files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f026c30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Files\n",
    "\n",
    "We extracted the files from the primary objects we cared about, now lets filter the CSVs by searching each file of the type `Annotation` and then making sure we get a csv file by testing the `encodingFormat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3e409",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "annotations = set()\n",
    "csvs = list()\n",
    "\n",
    "# pick out the annotation files\n",
    "for file in files:\n",
    "    if 'Annotation' in as_list(file.type):\n",
    "        annotations.add(file)\n",
    "\n",
    "# from this annotations select only the CSVs\n",
    "\n",
    "for annotation in annotations:\n",
    "    if annotation.get('encodingFormat') == 'text/csv':\n",
    "        annotation_json = annotation.as_jsonld()\n",
    "        csvs.append(annotation_json)\n",
    "    \n",
    " \n",
    "print(f\"We have {len(csvs)} csv objects\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4607693",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## CSVs\n",
    "\n",
    "Lets explore the metadata of one csv object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ef062",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csv = next(iter(csvs))\n",
    "print(json.dumps(csv, indent=2, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172d945",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalize CSV objects\n",
    "\n",
    "Lets use pandas json_normalize to create a data frame for speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ba1b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csvs_dataframe = pd.json_normalize(csvs)\n",
    "    \n",
    "list(csvs_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab5e54",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "csvs_dataframe.iloc[0]['@id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb173c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Speakers\n",
    "\n",
    "Each RepositoryObject has speakers. Lets find them in a similar way we did with the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8948fa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "speakers = list()\n",
    "\n",
    "for entity in primary_object_types:\n",
    "    if 'speaker' in entity:\n",
    "        speaker = entity.get('speaker')\n",
    "        if speaker:\n",
    "            for person in as_list(speaker):\n",
    "                speaker_item = ldaca.crate.dereference(person['@id'])\n",
    "                speakers.append(speaker_item.as_jsonld())\n",
    "print(f\"{len(speakers)} speakers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98ad1e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Person\n",
    "Each speaker is represented by a `Person` object lets explore one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee3dd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "person = next(iter(speakers))\n",
    "print(json.dumps(person, indent=2, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5d97b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Normalize Speakers\n",
    "\n",
    "Lets use pandas json_normalize to create a data frame for speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ab159d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "speakers_dataframe = pd.json_normalize(speakers)\n",
    "    \n",
    "speakers_dataframe   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c79c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Lets use 'indexableText.@id' to join the csvs in the primary objects dataframe\n",
    "\n",
    "primary_objects_dataframe.iloc[195]['indexableText.@id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eeb76f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_1 = pd.merge(left=csvs_dataframe, right=primary_objects_dataframe, left_on='@id', right_on=\"indexableText.@id\",\n",
    "              suffixes=('_csvs', '_po'), how='left')\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c185770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now lets use speaker@id to join with the dataframe we have just created\n",
    "df_1.iloc[0]['speaker.@id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e0cb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new = pd.merge(left=speakers_dataframe, right=df_1, left_on='@id', right_on=\"speaker.@id\",\n",
    "              suffixes=('_speaker', '_artefact'), how='left')\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90383f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Statistical Summaries\n",
    "\n",
    "In the metadata there is a key called \"birthDate\" which is a string that only has the birth year of the speaker. One of the birthDate values in the metadata has a string value \"c 1924\", instead of a simply sequence of digits there is, as shown when the list of birthDates are printed\n",
    "\n",
    "### Birth Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3aece5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new.birthDate_speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e11b3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If 'birthDate' only has the year listed as a text string (str), we would need to convert the birthDate value from str to an integer (int) if we want to do any statistical operations based on birthDate. This conversion can be done by 'type casting', eg, if year is a string that has the value \"1924\", we simply impose the type int(), such as int(year), so the string year = \"1924\" => integer, year = 1924, which is then a number (not a string) that can undergo maths operations.\n",
    "\n",
    "The function int(...) in the following line imposes an integer type conversion. That is, int(x) converts x from whatever type it is into an integer, as long as it makes sense for x to be converted into an integer. For example if x = \"abc\", then it would be impossible to know what value x would have as an integer. But if a string x = \"1924\", then the integer would have the value 1924."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34c99a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalising the birth year and casting them as integers\n",
    "\n",
    "new.birthDate_speaker = new.birthDate_speaker.apply(lambda year: year if (type(year) == int) else int(year[-4:]))\n",
    "new.birthDate_speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6ac2d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: datetime</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "The library datetime can provide the current date and time and allows us to do calculations over any date and time, such as determining the difference between time zones.\n",
    "<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a49e82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import datetime\n",
    "\n",
    "# We can calculate the mean (average) age\n",
    "this_year = datetime.datetime.now().year\n",
    "# Create a list called 'age' which takes every year in birth_year as y. Then get this_year and minus that\n",
    "# number from year y and make sure all those numbers are stored in a list, which is why we have [] around the\n",
    "# whole sequence of instructions below.\n",
    "age = [this_year - y for y in new.birthDate_speaker]\n",
    "\n",
    "# Print the list of the age of all the speakers if they were all alive today\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e29df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: statistics</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "The statistics library provides functions to calculate simple statistics, such as the mean, mode, standard deviation, etc., over numeric data.<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9fb10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import statistics\n",
    "\n",
    "print('== AGE ==')\n",
    "# Print the mean age, which is the average age of all the speakers\n",
    "print('MEAN:', statistics.mean(age))\n",
    "# The mode is the most freqently occur age. That is, there are more speakers of this age than any other.\n",
    "print('MODE:', statistics.mode(age))\n",
    "# The median is the middle value if the age of the participants were listed in order.\n",
    "print('MEDIAN:', statistics.median(age))\n",
    "# The standard deviation is a statistical metric that gives us an indication of how dispersed the age range of the speakers is\n",
    "print('STD DEV:', \"{:.1f}\".format(statistics.stdev(age)))\n",
    "print()\n",
    "\n",
    "print('== BIRTH YEAR ==')\n",
    "# Print the mean, median, mode and standard deviation of the birth year\n",
    "print('MEAN:', statistics.mean(new.birthDate_speaker))\n",
    "print('MEDIAN:', statistics.median(new.birthDate_speaker))\n",
    "print('MODE:', statistics.mode(new.birthDate_speaker))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469449e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>The Counter Container</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "The Counter container monitors the number of equivalent elements that have been added to it. Learn more about it here: https://docs.python.org/3/library/collections.html#collections.Counter<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07f71c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Other Metadata Features: Place\n",
    "\n",
    "There are other metadata columns that require normalising in this dataframe. For example there is a location 'Penrith' as well as 'Kingston, Penrith', and there is a location 'St. Marys' as well as 'St Marys' (no '.'), as shown when you print the 'address' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e2662",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new.address_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1183e7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's normalise these locations within the dataframe 'new'\n",
    "# NOTE 'address' is the where the story told in the interview takes place\n",
    "new.address_speaker = new['address_speaker'].apply(lambda place: place.split(',')[-1].replace('.', '').strip())\n",
    "new.address_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf759ee",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "place_of_story = new.address_speaker\n",
    "place_of_birth = new.birthDate_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece013e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# How many of the interviews talked about a certain location/city/suburb\n",
    "count_story_place = dict(Counter(new.address_speaker))\n",
    "pprint.pprint(count_story_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173e592",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Count place of birth\n",
    "count_birth_place = dict(Counter(new.birthDate_speaker))\n",
    "pprint.pprint(count_birth_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f949cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross-cutting 2 features found in the metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524dcf8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try with 1 suburb, where the suburb = Blacktown\n",
    "suburb = new.loc[new['address_speaker'] == 'Penrith']\n",
    "# For all stories set in this suburb, print all the storytellers' birth year \n",
    "print(suburb.birthDate_speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19580cde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print place of story and speaker's birth year\n",
    "all_suburbs = list(count_story_place.keys())\n",
    "all_suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8818cb72",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = list(suburb.name)\n",
    "names = list( dict.fromkeys(names) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d37bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17fdba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Traverse through the suburbs and print the data we are interested in\n",
    "# In addition, let's save this information in a dictionary called 'suburbs'\n",
    "# so we don't have to bother with dataframes\n",
    "suburbs = dict()\n",
    "for s in all_suburbs:\n",
    "    suburbs[s] = dict()\n",
    "    places = new.loc[(new['address_speaker'] == s)]\n",
    "    print('## ===', s, '-- total:', len(places))\n",
    "    # NOTE: index is the internal reference for the row in the dataframe called 'p'\n",
    "    for index, i in places.iterrows():\n",
    "        # initialise each person's info\n",
    "        person = dict()\n",
    "        # name\n",
    "        name = i['name']\n",
    "        print(name)\n",
    "        # birthPlace\n",
    "        birthPlace = i['birthPlace_speaker']\n",
    "        person['birthPlace_speaker'] = birthPlace\n",
    "        print(birthPlace)\n",
    "        # birthDate\n",
    "        birthDate = i['birthDate_speaker']\n",
    "        person['birthDate_speaker'] = birthDate\n",
    "        print(birthDate)        \n",
    "        # dialogue files\n",
    "        person_files = []\n",
    "        print(person)\n",
    "        if i['indexableText.@id']:\n",
    "            person_files.append(i['indexableText.@id'])\n",
    "#             if f['@id']:\n",
    "#                 print(f['@id'])\n",
    "#                 if f['@id'].endswith('.csv'):\n",
    "#                     print(f['@id'])\n",
    "#                     person_files.append(f['@id'])\n",
    "        person['files'] = person_files\n",
    "        print(person_files)\n",
    "        print()\n",
    "        \n",
    "        suburbs[s].update({name: person})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8468ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pprint.pprint(suburbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1c9fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sanity Check\n",
    "\n",
    "Let's print out the information on 1 person to check that our data is looking as we expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f23f11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NAME = 'Patricia Parker'\n",
    "PLACE = 'Blacktown'\n",
    "\n",
    "# Print the whole dict structure for Amelia Vincent\n",
    "suburbs[PLACE][NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7287e95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the birthPlace\n",
    "suburbs[PLACE][NAME]['birthPlace_speaker']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923dd5f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Downloading a file from the ReST API</b> \n",
    "    <br>    \n",
    "    We have the reference of each file stored in LDaCA. \n",
    "    <br>    \n",
    "    We use pandas to download and attach the API_TOKEN to each request\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df72e48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# files are a list, so let's create a list of dataframes to save all the contents of each file\n",
    "dataframes = list()  # we have a list of files so let's save them as a list of dataframes\n",
    "for f in suburbs[PLACE][NAME]['files']:\n",
    "    df = pd.read_csv(f, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "    df.fillna('', inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# How many files are there in the list?\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16850d4e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There is only 1 file in the list. Actually there is only ever 1 file in every files list\n",
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e8460",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Counting BiGrams\n",
    "\n",
    "Let's use textacy and spacy to process the text from each of the files and count the bigrams.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Text Processing</b> \n",
    "<br>    \n",
    "<ul>\n",
    "    <li>textacy: to find bigrams</li>\n",
    "    <li>spacy: to ingest and process the text</li>\n",
    "</ul>    \n",
    "    \n",
    "<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb526f6b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import textacy\n",
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08067302",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blacktown = list()\n",
    "print('## == BLACKTOWN')\n",
    "for person in suburbs['Blacktown']:\n",
    "    person_data = suburbs['Blacktown'][person]\n",
    "    file = person_data['files'][0]\n",
    "    df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "    df.fillna('', inplace=True)\n",
    "    text = list(df.text)\n",
    "    blacktown.extend(text)\n",
    "    print(person)\n",
    "    print('\\tCUMULATIVE TOTAL', len(blacktown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483906b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "penrith = list()\n",
    "print('## == Penrith')\n",
    "for person in suburbs['Penrith']:\n",
    "    person_data = suburbs['Penrith'][person]\n",
    "    file = person_data['files'][0]\n",
    "    df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "    df.fillna('', inplace=True)\n",
    "    text = list(df.text)\n",
    "    penrith.extend(text)\n",
    "    print(person)\n",
    "    print('\\tCUMULATIVE TOTAL', len(penrith))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd7d35",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_b = nlp(' '.join(blacktown))\n",
    "ngrams_b = list(textacy.extract.basics.ngrams(text_b, 2, min_freq=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcb2b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words_b = [w.text.lower() for w in ngrams_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac705d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cb = Counter(words_b)\n",
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600844f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_p = nlp(' '.join(penrith))\n",
    "ngrams_p = list(textacy.extract.basics.ngrams(text_p, 2, min_freq=10))\n",
    "words_p = [w.text.lower() for w in ngrams_p]\n",
    "cp = Counter(words_p)\n",
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bc644",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "overlapping = [w for w in cp if w in cb]\n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409fc340",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_blacktown = [w for w in cb if w not in cp]\n",
    "unique_blacktown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec51dda",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_penrith = [w for w in cp if w not in cb]\n",
    "unique_penrith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0eaeae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "birth_year_blacktown = [suburbs['Blacktown'][name]['birthDate_speaker'] for name in suburbs['Blacktown']]\n",
    "birth_year_blacktown.sort()\n",
    "birth_year_blacktown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b014a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "birth_year_penrith = [suburbs['Penrith'][name]['birthDate_speaker'] for name in suburbs['Penrith']]\n",
    "birth_year_penrith.sort()\n",
    "birth_year_penrith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e96296",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Counting n-grams for Suburb given Birth Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34978cd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "blacktown_pre20s = list()\n",
    "print('## == BLACKTOWN PRE-1920')\n",
    "for person in suburbs['Blacktown']:\n",
    "    person_data = suburbs['Blacktown'][person]\n",
    "    if person_data['birthDate_speaker'] < 1920:\n",
    "        file = person_data['files'][0]\n",
    "        df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "        df.fillna('', inplace=True)\n",
    "        text = list(df.text)\n",
    "        blacktown_pre20s.extend(text)\n",
    "        print(person)\n",
    "        print('\\tCUMULATIVE TOTAL', len(blacktown_pre20s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2b499",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "penrith_pre20s = list()\n",
    "print('## == PENRITH PRE-1920')\n",
    "for person in suburbs['Penrith']:\n",
    "    person_data = suburbs['Penrith'][person]\n",
    "    if person_data['birthDate_speaker'] < 1920:\n",
    "        file = person_data['files'][0]\n",
    "        df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "        df.fillna('', inplace=True)\n",
    "        text = list(df.text)\n",
    "        penrith_pre20s.extend(text)\n",
    "        print(person)\n",
    "        print('\\tCUMULATIVE TOTAL', len(penrith_pre20s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abee407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_b20 = nlp(' '.join(blacktown_pre20s))\n",
    "ngrams_b20 = list(textacy.extract.basics.ngrams(text_b20, 2, min_freq=10))\n",
    "words_b20 = [w.text.lower() for w in ngrams_b20]\n",
    "cb20 = Counter(words_b20)\n",
    "cb20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635b3bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_p20 = nlp(' '.join(penrith_pre20s))\n",
    "ngrams_p20 = list(textacy.extract.basics.ngrams(text_p20, 2, min_freq=10))\n",
    "words_p20 = [w.text.lower() for w in ngrams_p20]\n",
    "cp20 = Counter(words_p20)\n",
    "cp20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafc24e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>You Can Extend this Notebook</b> \n",
    "    \n",
    "<ul>\n",
    "<li> You change this notebook studying different suburbs.</li>\n",
    "<li> Rather than examining the vocabulary of those born before 1920, you can look at the stories of those who were born later.</li>\n",
    "<li> Try looking at unigrams or trigrams instead of bigrams.</li>\n",
    "<li> The minimum frequency of bigrams was 10. You can increase or decrease this threshold.</li>\n",
    "</ul>    \n",
    "<br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "description": "A sample notebook for the Farms to Freeways data",
  "input": "arcp://name,farms-to-freeways/corpus/root",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "title": "Farms to freeways notebook"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
