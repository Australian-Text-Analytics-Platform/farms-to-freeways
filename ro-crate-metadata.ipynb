{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a649fe41",
   "metadata": {},
   "source": [
    "# Loading Farms to Freeways from the API and ro-crate metadata file\n",
    "\n",
    "The Language Data Commons of Australia (LDaCa) packages all their data collections in an [ro-crate](https://www.researchobject.org/ro-crate/). There is a metadata file called `ro-crate-metadata.json` that comes with every data collection and this is how we can obtain metadata on this collection of research objects.\n",
    "\n",
    "The metadata file is in the json format, and so we'll be learning how to read a json file in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d0ea0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Skills</b> \n",
    "    \n",
    "<ul>\n",
    "<li> json file format (see https://en.wikipedia.org/wiki/JSON)</li>\n",
    "<li> working with dataframes, via pandas</li>\n",
    "<li> discovering and exploring metadata</li>\n",
    "<li> extracting ngrams, via textacy</li>\n",
    "</ul>    \n",
    "<br>\n",
    "\n",
    "<b>Skill level:</b> Intermediate\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0638388c",
   "metadata": {},
   "source": [
    "This notebook uses the library 'requests', as shown in the [Using APIs: Open Australia](https://github.com/Australian-Text-Analytics-Platform/open-australia-api/blob/main/api.ipynb) notebook. If you haven't already familiarised yourself with that notebook, it might be a good idea to do so first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86767334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we begin, let's make sure that we install all the requirements that we need\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a74abd",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Python needs the libraries that will be used by the notebook to be specified before they are used. We do this with the reserved word `import`, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e66c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json                       # json library to read json file formats\n",
    "import pprint                     # Prints in a nice way\n",
    "import requests                   # Uses the requests library for REST apis\n",
    "import os                         # Loads operating system libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ff0a7",
   "metadata": {},
   "source": [
    "## Variables \n",
    "\n",
    "We need to specify the path to the data collection. This `Farms to Freeways` data collection was used with permission by [The University of Western Sydney](https://omeka.uws.edu.au/farmstofreeways/). It was made into an ro-crate by the [LDaCa](https://ardc.edu.au/project/language-data-commons-of-australia-ldaca/) project and it is the data set used here to demonstrate the skills list above.\n",
    "\n",
    "The variables below refer to the `path` where the collection can be found. There are also variables below that refer to ro-crates as specified in LDaCa profiles, for example all artefacts of importances are called `RepositoryObject`, and when an artefact is linked to, it is done with a `hadFile` keyword in the ro-crate metadata file.\n",
    "\n",
    "Create a file name it vars.env and store your API_KEY. This will be required for downloading any files. Go to [LDACA website](https://oni-demo.text-commons.org) and generate an API Key\n",
    "\n",
    "Example:\n",
    "```\n",
    "API_KEY=12345\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ff583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify location where collection is\n",
    "LDACA_API = 'https://oni-demo.text-commons.org/api/data'\n",
    "COLLECTION_ID = 'arcp://name,farms-to-freeways/root/description'\n",
    "from dotenv import load_dotenv    # loads environment variables\n",
    "load_dotenv('vars.env') # load the environment variables located in the vars.env files\n",
    "API_TOKEN = os.getenv('API_KEY') # store your environment variable in this jupyter notebook\n",
    "if not API_TOKEN:\n",
    "    print(\"Set a variable in the vars.env file and name API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb53776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Farms to Freeways ro-crate metadata by passing the arcpId in as a parameter to the get request\n",
    "params = dict()\n",
    "params['id'] = COLLECTION_ID\n",
    "\n",
    "f2f_response = requests.get(LDACA_API, params=params)\n",
    "metadata = f2f_response.json()\n",
    "\n",
    "# Inspect the metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fc57b",
   "metadata": {},
   "source": [
    "### ro-crate Profiles\n",
    "\n",
    "An ro-crate profile is a set of conventions that tell us what elements an ro-crate minimally contains.\n",
    "\n",
    "These profiles tell us what to expect to find in the data packages. Learn more about them here: https://www.researchobject.org/ro-crate/profiles.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords from LDaCa ro-crate profiles\n",
    "OBJECT_LINKAGE = 'hasFile'\n",
    "GRAPH = '@graph'\n",
    "TYPE = '@type'\n",
    "ID = '@id'\n",
    "\n",
    "# TYPE values are lists. \n",
    "# We define a PRIMARY_OBJECT as a 'RepositoryObject' because that is where the main data is stored \n",
    "PRIMARY_OBJECT = 'RepositoryObject'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc7f87",
   "metadata": {},
   "source": [
    "### Define Variables to Gather Metadata\n",
    "\n",
    "As suggested above, there are '@types' that define certain objects within the collection, for example there is type called 'Person'. This json object stores information such as 'birthDate' about the 'Person'. In the code block below, we discover all the types stored in this metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all types and find types that have linked objects\n",
    "linked_objects = set()\n",
    "types = list()\n",
    "primary_object_types = set()\n",
    "\n",
    "# Traverse through all the objects in the metadata file\n",
    "for entity in metadata[GRAPH]:\n",
    "    my_type = entity[TYPE]\n",
    "    if type(my_type) == str:\n",
    "        my_type = [my_type]\n",
    "    if my_type not in types:\n",
    "        types.append(my_type)\n",
    "        \n",
    "    # [PRIMARY_OBJECT, X] : primary_object_type = X\n",
    "    if PRIMARY_OBJECT in my_type:\n",
    "        primary_object_type = [e for e in my_type if e not in [PRIMARY_OBJECT]][0]\n",
    "        primary_object_types.add(primary_object_type)\n",
    "\n",
    "        if OBJECT_LINKAGE in entity:\n",
    "            for x in entity[OBJECT_LINKAGE]:\n",
    "                filename = x[ID]\n",
    "                suffix = filename.split('.')[-1]\n",
    "                if suffix not in linked_objects:\n",
    "                    linked_objects.add((suffix, primary_object_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d7f74",
   "metadata": {},
   "source": [
    "### Flatten a list of lists using itertools\n",
    "\n",
    "The variable 'types' above is a list of lists and we will flatten it with itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84b12",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: itertools</b> \n",
    "    \n",
    "The itertools library allows you to iterate over lists without having list comprehension lines of explicit \"for loops\" in your code. A and B are equivalent to the code below.\n",
    "\n",
    "<br> \n",
    "    \n",
    "   \n",
    "(A)&emsp; flat_types = [t for sublist in types for t in sublist] \n",
    "\n",
    "<br>  \n",
    "\n",
    "(B) &emsp;flat_types = list()\n",
    "<br>\n",
    "&emsp;&emsp;&emsp;for sublist in types: \n",
    "<br>\n",
    "&emsp;&emsp; &emsp;&emsp;   for t in sublist:\n",
    "<br>\n",
    "&emsp;&emsp;&emsp; &emsp;&emsp;       flat_types.append(t) </br>\n",
    "</div>\n",
    "\n",
    "The following line of code does the following:\n",
    "    [[a, b], [c, d]] ==> [a, b, c, d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools     # `types` above is a list of lists and we will flatten it with itertools\n",
    "\n",
    "flat_types = list(itertools.chain.from_iterable(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613ed17",
   "metadata": {},
   "source": [
    "## Exploring the Metadata\n",
    "\n",
    "Anytime you work with data, it's always a good idea to inspect it by printing it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f64dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the variables\n",
    "# All the types\n",
    "pprint.pp(sorted(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a61f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the unique types\n",
    "pprint.pp(set(flat_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae67a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of PRIMARY_OBJECTs ie [PRIMARY_OBJECT, X]. What kinds of Xs do we have?\n",
    "print(primary_object_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb7f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the research artefacts/files their types\n",
    "pprint.pp(linked_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a041dd",
   "metadata": {},
   "source": [
    "## Primary Objects\n",
    "\n",
    "The primary object types are the ones we may care about, so we will pull them into their own dataframe:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94f3aa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: pandas (dataframe)</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "A dataframe is akin to a table -- it is made up of rows and columns. \n",
    "    \n",
    "<br>    \n",
    "In the block of code below, we are creating a dataframe for each \"primary_object_type\" ('Person', 'TextDialogue', 'Photographic image', and 'Text')\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e3e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # this means we will refer to pandas as 'pd' throughout the code\n",
    "\n",
    "all_data = dict()\n",
    "\n",
    "# traverse over all of the metadata\n",
    "for entity in metadata[GRAPH]:\n",
    "    for t in entity[TYPE]:\n",
    "        if t in primary_object_types:\n",
    "            df = pd.json_normalize(entity)\n",
    "            if t not in all_data:\n",
    "                all_data[t] = df\n",
    "            else:\n",
    "                all_data[t] = pd.concat([all_data[t], df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first TextDialogue item \n",
    "text_dialogue = all_data['TextDialogue']\n",
    "\n",
    "print(text_dialogue.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the primary object Person with @id #568\n",
    "person = all_data['Person']\n",
    "\n",
    "person.loc[person['@id'] == '#568']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = pd.merge(left=all_data['TextDialogue'], right=all_data['Person'], left_on=\"speaker.@id\", right_on=\"@id\",\n",
    "               suffixes=('_artefact', '_speaker'), how='inner')\n",
    "\n",
    "# Print the new dataframe\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90383f7",
   "metadata": {},
   "source": [
    "## Statistical Summaries\n",
    "\n",
    "In the metadata there is a key called \"birthDate\" which is a string that only has the birth year of the speaker. One of the birthDate values in the metadata has a string value \"c 1924\", instead of a simply sequence of digits there is, as shown when the list of birthDates are printed\n",
    "\n",
    "### Birth Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab69034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the birth year of all the interviewees\n",
    "new.birthDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d776856",
   "metadata": {},
   "source": [
    "The value \"c 1924\" needs to be normalised as a regular looking year, ie 4 numbers in a sequence. This can be done by simply only allowing the last 4 characters in any string that is a birthDate. For example if year0 = \"1918\" and year1 = \"c 1924\" and we only take the last 4 characters, then year0[-4:] = \"1918\" and year1[-4:] = \"1924\".\n",
    "\n",
    "'birthDate' only has the year listed as a text string (str), therefore we need to convert the birthDate value from str to an integer (int) if we want to do any statistical operations based on birthDate. This conversion can be done by 'type casting', eg, if year is a string that has the value \"1924\", we simply impose the type int(), such as int(year), so the string year = \"1924\" => integer, year = 1924, which is then a number (not a string) that can undergo maths operations.\n",
    "\n",
    "The function int(...) in the following line imposes an integer type conversion. That is, int(x) converts x from whatever type it is into an integer, as long as it makes sense for x to be converted into an integer. For example if x = \"abc\", then it would be impossible to know what value x would have as an integer. But if a string x = \"1924\", then the integer would have the value 1924.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7feefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the birth year and casting them as integers\n",
    "new['birthDate'] = new['birthDate'].apply(lambda year: int(year[-4:]))\n",
    "new.birthDate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6ac2d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: datetime</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "The library datetime can provide the current date and time and allows us to do calculations over any date and time, such as determining the difference between time zones.\n",
    "<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a49e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import datetime\n",
    "\n",
    "# We can calculate the mean (average) age\n",
    "this_year = datetime.datetime.now().year\n",
    "# Create a list called 'age' which takes every year in birth_year as y. Then get this_year and minus that\n",
    "# number from year y and make sure all those numbers are stored in a list, which is why we have [] around the\n",
    "# whole sequence of instructions below.\n",
    "age = [this_year - y for y in new.birthDate]\n",
    "\n",
    "# Print the list of the age of all the speakers if they were all alive today\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e29df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Python Library: statistics</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "The statistics library provides functions to calculate simple statistics, such as the mean, mode, standard deviation, etc., over numeric data.<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d9fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the module\n",
    "import statistics\n",
    "\n",
    "print('== AGE ==')\n",
    "# Print the mean age, which is the average age of all the speakers\n",
    "print('MEAN:', statistics.mean(age))\n",
    "# The mode is the most freqently occur age. That is, there are more speakers of this age than any other.\n",
    "print('MODE:', statistics.mode(age))\n",
    "# The median is the middle value if the age of the participants were listed in order.\n",
    "print('MEDIAN:', statistics.median(age))\n",
    "# The standard deviation is a statistical metric that gives us an indication of how dispersed the age range of the speakers is\n",
    "print('STD DEV:', \"{:.1f}\".format(statistics.stdev(age)))\n",
    "print()\n",
    "\n",
    "print('== BIRTH YEAR ==')\n",
    "# Print the mean, median, mode and standard deviation of the birth year\n",
    "print('MEAN:', statistics.mean(new.birthDate))\n",
    "print('MEDIAN:', statistics.median(new.birthDate))\n",
    "print('MODE:', statistics.mode(new.birthDate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469449e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>The Counter Container</b> \n",
    "\n",
    "<br>    \n",
    "    \n",
    "The Counter container monitors the number of equivalent elements that have been added to it. Learn more about it here: https://docs.python.org/3/library/collections.html#collections.Counter<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07f71c",
   "metadata": {},
   "source": [
    "### Other Metadata Features: Place\n",
    "\n",
    "There are other metadata columns that require normalising in this dataframe. For example there is a location 'Penrith' as well as 'Kingston, Penrith', and there is a location 'St. Marys' as well as 'St Marys' (no '.'), as shown when you print the 'address' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1183e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's normalise these locations within the dataframe 'new'\n",
    "# NOTE 'address' is the where the story told in the interview takes place\n",
    "new['address'] = new['address'].apply(lambda place: place.split(',')[-1].replace('.', '').strip())\n",
    "new.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf759ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "place_of_story = new.address\n",
    "place_of_birth = new.birthPlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the interviews talked about a certain location/city/suburb\n",
    "count_story_place = dict(Counter(new.address))\n",
    "pprint.pp(count_story_place, sort_dicts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count place of birth\n",
    "count_birth_place = dict(Counter(new.birthPlace))\n",
    "pprint.pp(count_birth_place, sort_dicts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f949cc",
   "metadata": {},
   "source": [
    "### Cross-cutting 2 features found in the metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with 1 suburb, where the suburb = Quakers Hill\n",
    "suburb = new.loc[new['address'] == 'Quakers Hill']\n",
    "# For all stories set in this suburb, print all the storytellers' birth year \n",
    "print(suburb.birthDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463410bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print place of story and speaker's birth year\n",
    "all_suburbs = list(count_story_place.keys())\n",
    "all_suburbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b995d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(suburb.name_speaker)\n",
    "suburb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f345d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "place1 = suburb.loc[(suburb['address'] == all_suburbs[2])]\n",
    "place1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6279fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "place2 = suburb.loc[(suburb['address'] == all_suburbs[2]) & (suburb['name_speaker'] == names[0])]\n",
    "place2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverse through the suburbs and print the data we are interested in\n",
    "# In addition, let's save this information in a dictionary called 'suburbs'\n",
    "# so we don't have to bother with dataframes\n",
    "suburbs = dict()\n",
    "for s in all_suburbs:\n",
    "    suburbs[s] = dict()\n",
    "    places = new.loc[(new['address'] == s)]\n",
    "    print('## ===', s, '-- total:', len(places))\n",
    "    # NOTE: index is the internal reference for the row in the dataframe called 'p'\n",
    "    for index, i in places.iterrows():\n",
    "        # initialise each person's info\n",
    "        person = dict()\n",
    "        # name\n",
    "        name = i['name_speaker']\n",
    "        print(name)\n",
    "        # birthPlace\n",
    "        birthPlace = i['birthPlace']\n",
    "        person['birthPlace'] = birthPlace\n",
    "        print(birthPlace)\n",
    "        # birthDate\n",
    "        birthDate = i['birthDate']\n",
    "        person['birthDate'] = birthDate\n",
    "        print(birthDate)        \n",
    "        # dialogue files\n",
    "        files = [f['@id'] for f in i['hasFile'] if f['@id'].endswith('.csv')]\n",
    "        person['files'] = files\n",
    "        print(files)\n",
    "        print()\n",
    "        \n",
    "        suburbs[s].update({name: person})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8468ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(suburbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c1c9fb",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "\n",
    "Let's print out the information on 1 person to check that our data is looking as we expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f23f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = 'Amelia Vincent'\n",
    "PLACE = 'Blacktown'\n",
    "\n",
    "# Print the whole dict structure for Amelia Vincent\n",
    "suburbs[PLACE][NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7287e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the birthPlace\n",
    "suburbs[PLACE][NAME]['birthPlace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df72e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files are a list, so let's create a list of dataframes to save all the contents of each file\n",
    "dataframes = list()  # we have a list of files so let's save them as a list of dataframes\n",
    "for f in suburbs[PLACE][NAME]['files']:\n",
    "    df = pd.read_csv(f, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "    df.fillna('', inplace=True)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# How many files are there in the list?\n",
    "len(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16850d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is only 1 file in the list. Actually there is only ever 1 file in every files list\n",
    "dataframes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e8460",
   "metadata": {},
   "source": [
    "## Counting BiGrams\n",
    "\n",
    "Let's use textacy and spacy to process the text from each of the files and count the bigrams.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Text Processing</b> \n",
    "<br>    \n",
    "<ul>\n",
    "    <li>textacy: to find bigrams</li>\n",
    "    <li>spacy: to ingest and process the text</li>\n",
    "</ul>    \n",
    "    \n",
    "<br>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb526f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08067302",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacktown = list()\n",
    "print('## == BLACKTOWN')\n",
    "for person in suburbs['Blacktown']:\n",
    "    person_data = suburbs['Blacktown'][person]\n",
    "    file = person_data['files'][0]\n",
    "    df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "    df.fillna('', inplace=True)\n",
    "    text = list(df.text)\n",
    "    text.remove('')\n",
    "    blacktown.extend(text)\n",
    "    print(person)\n",
    "    print('\\tCUMULATIVE TOTAL', len(blacktown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f483906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "penrith = list()\n",
    "print('## == Penrith')\n",
    "for person in suburbs['Penrith']:\n",
    "    person_data = suburbs['Penrith'][person]\n",
    "    file = person_data['files'][0]\n",
    "    df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "    df.fillna('', inplace=True)\n",
    "    text = list(df.text)\n",
    "    text.remove('')\n",
    "    penrith.extend(text)\n",
    "    print(person)\n",
    "    print('\\tCUMULATIVE TOTAL', len(penrith))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_b = nlp(' '.join(blacktown))\n",
    "ngrams_b = list(textacy.extract.basics.ngrams(text_b, 2, min_freq=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_b = [w.text.lower() for w in ngrams_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cb = Counter(words_b)\n",
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_p = nlp(' '.join(penrith))\n",
    "ngrams_p = list(textacy.extract.basics.ngrams(text_p, 2, min_freq=10))\n",
    "words_p = [w.text.lower() for w in ngrams_p]\n",
    "cp = Counter(words_p)\n",
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping = [w for w in cp if w in cb]\n",
    "overlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409fc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_blacktown = [w for w in cb if w not in cp]\n",
    "unique_blacktown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec51dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_penrith = [w for w in cp if w not in cb]\n",
    "unique_penrith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0eaeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_year_blacktown = [suburbs['Blacktown'][name]['birthDate'] for name in suburbs['Blacktown']]\n",
    "birth_year_blacktown.sort()\n",
    "birth_year_blacktown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b014a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_year_penrith = [suburbs['Penrith'][name]['birthDate'] for name in suburbs['Penrith']]\n",
    "birth_year_penrith.sort()\n",
    "birth_year_penrith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e96296",
   "metadata": {},
   "source": [
    "### Counting n-grams for Suburb given Birth Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34978cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "blacktown_pre20s = list()\n",
    "print('## == BLACKTOWN PRE-1920')\n",
    "for person in suburbs['Blacktown']:\n",
    "    person_data = suburbs['Blacktown'][person]\n",
    "    if person_data['birthDate'] < 1920:\n",
    "        file = person_data['files'][0]\n",
    "        df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "        df.fillna('', inplace=True)\n",
    "        text = list(df.text)\n",
    "        text.remove('')\n",
    "        blacktown_pre20s.extend(text)\n",
    "        print(person)\n",
    "        print('\\tCUMULATIVE TOTAL', len(blacktown_pre20s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "penrith_pre20s = list()\n",
    "print('## == PENRITH PRE-1920')\n",
    "for person in suburbs['Penrith']:\n",
    "    person_data = suburbs['Penrith'][person]\n",
    "    if person_data['birthDate'] < 1920:\n",
    "        file = person_data['files'][0]\n",
    "        df = pd.read_csv(file, storage_options={'Authorization': 'Bearer %s' % API_TOKEN})\n",
    "        df.fillna('', inplace=True)\n",
    "        text = list(df.text)\n",
    "        text.remove('')\n",
    "        penrith_pre20s.extend(text)\n",
    "        print(person)\n",
    "        print('\\tCUMULATIVE TOTAL', len(penrith_pre20s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abee407",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_b20 = nlp(' '.join(blacktown_pre20s))\n",
    "ngrams_b20 = list(textacy.extract.basics.ngrams(text_b20, 2, min_freq=10))\n",
    "words_b20 = [w.text.lower() for w in ngrams_b20]\n",
    "cb20 = Counter(words_b20)\n",
    "cb20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_p20 = nlp(' '.join(penrith_pre20s))\n",
    "ngrams_p20 = list(textacy.extract.basics.ngrams(text_p20, 2, min_freq=10))\n",
    "words_p20 = [w.text.lower() for w in ngrams_p20]\n",
    "cp20 = Counter(words_p20)\n",
    "cp20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafc24e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>You Can Extend this Notebook</b> \n",
    "    \n",
    "<ul>\n",
    "<li> You change this notebook studying different suburbs.</li>\n",
    "<li> Rather than examining the vocabulary of those born before 1920, you can look at the stories of those who were born later.</li>\n",
    "<li> Try looking at unigrams or trigrams instead of bigrams.</li>\n",
    "<li> The minimum frequency of bigrams was 10. You can increase or decrease this threshold.</li>\n",
    "</ul>    \n",
    "<br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
